{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db9aacb-6ff0-492c-bb4c-110878625f30",
   "metadata": {},
   "source": [
    "# Kafka Subscriber Task\n",
    "- Import the SparkSession class and alias it as ss\n",
    "- Import StructType and StructField for defining DataFrame schemas\n",
    "- Import necessary Spark SQL functions for data processing and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a28580a-c002-4015-ac08-692a09edfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as ss\n",
    "from pyspark.sql.types import (\n",
    "    StructType as st,\n",
    "    StructField as sf,\n",
    "    StringType as srt,\n",
    "    DoubleType as dt,\n",
    "    LongType as lt\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    from_json,       # Parses a JSON string column into a struct\n",
    "    col,             # Selects a column\n",
    "    to_date,         # Converts a timestamp string to a date\n",
    "    to_timestamp,    # Converts a string to a timestamp\n",
    "    current_date,    # Gets the current date\n",
    "    current_timestamp,# Gets the current timestamp\n",
    "    lit,             # Creates a literal value column\n",
    "    map_from_arrays, # Creates a map column from two array columns (keys and values)\n",
    "    array,           # Creates an array column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba2ccde-2d65-406a-beb7-be0a65cb5d0d",
   "metadata": {},
   "source": [
    "Build a SparkSession instance configured for Kafka and Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e8e56db-a4ab-4099-b477-7119af4b1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprk = ss.builder \\\n",
    "    .appName(\"KafkaSubscriber\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        # Include packages for Kafka SQL connector and Delta Lake integration\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5,\"\n",
    "        \"io.delta:delta-spark_2.12:3.3.0\"\n",
    "    ) \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org\") \\\n",
    "    # Enable Delta Lake SQL extensions for the Spark session\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    # Configure the Spark catalog to use the Delta Catalog implementation\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\")\\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .getOrCreate() # Get or create the SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cba25d-d1de-4b5e-9090-d6ee0766e878",
   "metadata": {},
   "source": [
    "Define the schema expected for the JSON messages coming from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac120d09-c8cf-4d40-9b94-044b1b53ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = st([\n",
    "    sf(\"Date/Time\", srt(), True),                  # String field for date/time\n",
    "    sf(\"LV ActivePower (kW)\", dt(), True),        # Double field for active power\n",
    "    sf(\"Wind Speed (m/s)\", dt(), True),            # Double field for wind speed\n",
    "    sf(\"Theoretical_Power_Curve (KWh)\", dt(), True), # Double field for theoretical power\n",
    "    sf(\"Wind Direction (°)\", dt(), True),          # Double field for wind direction\n",
    "    sf(\"row_id\", lt(), True)                       # Long field for a row identifier\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee64069-4745-4390-a5f7-34b98b889f60",
   "metadata": {},
   "source": [
    "Define the Kafka stream reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bc65c2-d2c5-4dd0-9c0e-26e20eaa7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = sprk.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"xenon-topic\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load() # Load data from the specified Kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf36ebe-22eb-45fa-b952-2eb15d9ed5dd",
   "metadata": {},
   "source": [
    "Process the raw Kafka data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a55182-90da-402f-8cf8-992ddb08499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kafka_df.select(\n",
    "    # Parse the 'value' column (which is binary) by first casting it to string\n",
    "    # and then applying the predefined json_schema\n",
    "    from_json(\n",
    "        col(\"value\").cast(\"string\"), # Cast the Kafka message value to a string\n",
    "        json_schema                 # Apply the schema to parse the JSON\n",
    "    ).alias(\"jsonData\")             # Alias the resulting struct column as \"jsonData\"\n",
    ").select(\"jsonData.*\")              # Flatten the struct to get individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd506785-faea-4f53-985b-2dfa13dc74e0",
   "metadata": {},
   "source": [
    "Transform the parsed data into the final desired structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53da54e-2aa3-4db6-9f5e-e5e5fdd43daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.select(\n",
    "    # Convert the 'Date/Time' string first to a timestamp, then extract the date\n",
    "    to_date(\n",
    "        to_timestamp(col(\"Date/Time\"), \"dd MM yyyy HH:mm\"),\n",
    "        \"yyyy-MM-dd\"\n",
    "    ).alias(\"signal_date\"),\n",
    "    \n",
    "    # Convert the 'Date/Time' string to a proper timestamp type\n",
    "    to_timestamp(col(\"Date/Time\"), \"dd MM yyyy HH:mm\").alias(\"signal_ts\"), # Alias as 'signal_ts'\n",
    "    \n",
    "    # Add the current date when the record is processed\n",
    "    current_date().alias(\"create_date\"),\n",
    "    \n",
    "    # Add the current timestamp when the record is processed\n",
    "    current_timestamp().alias(\"create_ts\"),\n",
    "    \n",
    "    # Create a map column containing the signal names as keys and their values as strings\n",
    "    map_from_arrays(\n",
    "        # Array of literal string keys (signal names)\n",
    "        array(\n",
    "            lit(\"LV ActivePower (kW)\"),\n",
    "            lit(\"Wind Speed (m/s)\"),\n",
    "            lit(\"Theoretical_Power_Curve (KWh)\"),\n",
    "            lit(\"Wind Direction (°)\")\n",
    "        ),\n",
    "        \n",
    "        # Array of corresponding signal values, cast to string\n",
    "        array(\n",
    "            col(\"LV ActivePower (kW)\").cast(\"string\"),\n",
    "            col(\"Wind Speed (m/s)\").cast(\"string\"),\n",
    "            col(\"Theoretical_Power_Curve (KWh)\").cast(\"string\"),\n",
    "            col(\"Wind Direction (°)\").cast(\"string\")\n",
    "        )\n",
    "    ).alias(\"signals\") # Alias the resulting map column as 'signals'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e97f0c7-d562-4f36-9fae-c38250378c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- signal_date: date (nullable = true)\n",
      " |-- signal_ts: timestamp (nullable = true)\n",
      " |-- create_date: date (nullable = false)\n",
      " |-- create_ts: timestamp (nullable = false)\n",
      " |-- signals: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322262a7-cbde-48a6-97c9-61a551cbcd61",
   "metadata": {},
   "source": [
    "Define and start the stream writer to save the data to a Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f95002-e1ac-40fb-8329-00be5e813ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7c23a0282810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/delta_kafka_subscriber_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(\"/data/delta_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf43420-5b1f-44a2-a40c-f44e483e255c",
   "metadata": {},
   "source": [
    "Read the data from the Delta table as a batch DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6305b1-233b-48e7-aa92-54b2574a7764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+--------------------+--------------------+\n",
      "|signal_date|          signal_ts|create_date|           create_ts|             signals|\n",
      "+-----------+-------------------+-----------+--------------------+--------------------+\n",
      "| 2018-01-01|2018-01-01 00:00:00| 2025-03-27|2025-03-27 03:00:...|{LV ActivePower (...|\n",
      "| 2018-01-01|2018-01-01 00:00:00| 2025-03-27|2025-03-27 03:00:...|{LV ActivePower (...|\n",
      "| 2018-01-01|2018-01-01 00:10:00| 2025-03-27|2025-03-27 03:00:...|{LV ActivePower (...|\n",
      "| 2018-01-01|2018-01-01 00:10:00| 2025-03-27|2025-03-27 03:00:...|{LV ActivePower (...|\n",
      "| 2018-01-01|2018-01-01 00:20:00| 2025-03-27|2025-03-27 03:00:...|{LV ActivePower (...|\n",
      "+-----------+-------------------+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sprk.read.format(\"delta\").load(\"/data/delta_output\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c1dfb0-1d3d-4de8-b392-dfd3c433dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprk.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
