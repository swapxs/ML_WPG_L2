{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ecd3aec-00c5-448d-86d9-89566340dfa7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Reads a static CSV file containing wind power data (with columns such as Date/Time, LV ActivePower (kW), etc.) using Spark.\n",
    "2. Assigns a sequential index (row_id) to each row in the CSV.\n",
    "3. Uses a rate streaming source to emit rows at a controlled rate.\n",
    "4. Joins the streaming source with the static DataFrame on row_id.\n",
    "5. Publishes the resulting joined rows to a Kafka topic (xenon-topic).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65ca608-d5f1-4d73-9029-5a45698b8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as ss, Row as r\n",
    "from pyspark.sql.functions import to_json, struct, col, lit\n",
    "from pyspark.sql.types import LongType as lt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a646b-2182-47e6-af35-e755b3a99f90",
   "metadata": {},
   "source": [
    "Initialize Spark Session:\n",
    "- Name the application `KafkaProducer`\n",
    "- Connect to the Spark master at `spark://spark-master:7077`\n",
    "- Include the spark-sql-kafka package so Spark can write to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5340a99-f9cc-4156-94a1-a48affa4847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprk = ss.builder \\\n",
    "    .appName(\"KafkaProducer\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5\") \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd6e26-c347-4230-a9ed-45c97ab772e6",
   "metadata": {},
   "source": [
    "File path (inside the container) for the CSV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "205bb00d-ac29-4c23-bdf9-bc5c5a4f671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = \"/data/dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143db577-a49d-478e-b2cd-9cca80535b8b",
   "metadata": {},
   "source": [
    "Load the static CSV DataFrame with inferred schema  \n",
    "`header=True` means first row in CSV is treated as column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab5c1f96-3e99-47ad-a124-1a964cdf1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sprk.read.csv(fp, header=True, inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80dd5fa1-7fbd-48cd-a30d-eee8fd2f1435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+----------------+-----------------------------+------------------+\n",
      "|       Date/Time|LV ActivePower (kW)|Wind Speed (m/s)|Theoretical_Power_Curve (KWh)|Wind Direction (°)|\n",
      "+----------------+-------------------+----------------+-----------------------------+------------------+\n",
      "|01 01 2018 00:00|   380.047790527343|5.31133604049682|             416.328907824861|  259.994903564453|\n",
      "|01 01 2018 00:10|    453.76919555664|5.67216682434082|             519.917511061494|   268.64111328125|\n",
      "|01 01 2018 00:20|   306.376586914062|5.21603679656982|             390.900015810951|  272.564788818359|\n",
      "|01 01 2018 00:30|   419.645904541015|5.65967416763305|             516.127568975674|  271.258087158203|\n",
      "|01 01 2018 00:40|   380.650695800781|5.57794094085693|             491.702971953588|  265.674285888671|\n",
      "+----------------+-------------------+----------------+-----------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e56b1-86b7-473b-bbe3-34a90b828dde",
   "metadata": {},
   "source": [
    "Function to add an index (row_id) to each row in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa536191-52b8-4c3e-8446-6c9245ca70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_idx_row(row, index):\n",
    "    row_dict = row.asDict() # Convert the Row to a dict, add \"row_id\", then rebuild a Row\n",
    "    row_dict[\"row_id\"] = index\n",
    "    return r(**row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad07dd-e2d9-4c1a-a981-0a2b446b841e",
   "metadata": {},
   "source": [
    "Convert the DataFrame to an RDD, zip each row with a unique index,  \n",
    "then map them back to a new row object that includes row_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42b8debf-fa83-4abb-955b-bb4372683a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd.zipWithIndex().map(lambda x: add_idx_row(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8719ff-3859-4f6d-8814-e8b044e888ac",
   "metadata": {},
   "source": [
    "Create a new schema based on the original schema plus row_id as a LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94730b04-12ec-4fd7-a532-ef7de7d88104",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_schema = df.schema.add(\"row_id\", lt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9e3731-f306-47c0-a87b-81146ce4edaf",
   "metadata": {},
   "source": [
    "Rebuild a Spark DataFrame with the new schema (including row_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e346a18-435f-48e0-8c0b-57491eef68c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonUtils.getBroadcastThreshold.\n: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.api.java.JavaSparkContext.sc()\" because \"jsc\" is null\n\tat org.apache.spark.api.java.JavaSparkContext$.toSparkContext(JavaSparkContext.scala:794)\n\tat org.apache.spark.api.python.PythonUtils$.getBroadcastThreshold(PythonUtils.scala:87)\n\tat org.apache.spark.api.python.PythonUtils.getBroadcastThreshold(PythonUtils.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_w_idx \u001b[38;5;241m=\u001b[39m \u001b[43msprk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_schema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/sql/session.py:1487\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1488\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m   1489\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/rdd.py:4918\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4915\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/rdd.py:5470\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5470\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5474\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5475\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5477\u001b[0m )\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/rdd.py:5268\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5267\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5269\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5271\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5272\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5277\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5278\u001b[0m )\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/rdd.py:5253\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5251\u001b[0m pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(command)\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetBroadcastThreshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5254\u001b[0m     \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[1;32m   5255\u001b[0m     broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n\u001b[1;32m   5256\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(broadcast)\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonUtils.getBroadcastThreshold.\n: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.api.java.JavaSparkContext.sc()\" because \"jsc\" is null\n\tat org.apache.spark.api.java.JavaSparkContext$.toSparkContext(JavaSparkContext.scala:794)\n\tat org.apache.spark.api.python.PythonUtils$.getBroadcastThreshold(PythonUtils.scala:87)\n\tat org.apache.spark.api.python.PythonUtils.getBroadcastThreshold(PythonUtils.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "df_w_idx = sprk.createDataFrame(rdd, schema=new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1604c864-e00f-4624-a210-170a3f054a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date/Time: string (nullable = true)\n",
      " |-- LV ActivePower (kW): double (nullable = true)\n",
      " |-- Wind Speed (m/s): double (nullable = true)\n",
      " |-- Theoretical_Power_Curve (KWh): double (nullable = true)\n",
      " |-- Wind Direction (°): double (nullable = true)\n",
      " |-- row_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_w_idx.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b9bbce-db03-4be1-9644-4beb048bebbe",
   "metadata": {},
   "source": [
    "Create a streaming DataFrame that generates data at a fixed rate.  \n",
    "Here, `rowsPerSecond=10` means `10` rows are produced every second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb34efc-1fcf-4c06-84ad-39881793103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = sprk.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73463dc7-8f8b-4cc9-bc2f-80790ff8320a",
   "metadata": {},
   "source": [
    "Rename the automatic `value` column (from `rate` source) to `row_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e6f2b55-8ae1-4371-bc9f-5779f0bb1953",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.withColumnRenamed(\"value\", \"row_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089ed4fd-62f7-47b3-b6f2-56a1ec0fa9a3",
   "metadata": {},
   "source": [
    "Count how many rows are in the static CSV DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b1703d6-9816-4db9-8cea-4aa051c3ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df_w_idx.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44e8424f-6430-4f9f-9f5b-b3cc07a02cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50530\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a1a11-982e-43c7-adbd-7be0b372a09d",
   "metadata": {},
   "source": [
    "Filter the streaming DataFrame so that it emits only row_ids < total row count  \n",
    "This ensures we don't exceed the number of rows in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd3a4476-e841-40b0-9263-6bd877d4bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.filter(col(\"row_id\") < lit(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bada6d-f51c-48d6-987b-4f542a04b143",
   "metadata": {},
   "source": [
    "Join the streaming DataFrame with the static DataFrame on row_id  \n",
    "`timestamp` is auto-generated by the rate source, so drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "330c6b0a-0228-41b9-8066-e3d08bdb4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = new_df.join(df_w_idx, on=\"row_id\", how=\"left_outer\").drop(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26581f0d-75f3-4c08-bf48-9dd59639bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: long (nullable = true)\n",
      " |-- Date/Time: string (nullable = true)\n",
      " |-- LV ActivePower (kW): double (nullable = true)\n",
      " |-- Wind Speed (m/s): double (nullable = true)\n",
      " |-- Theoretical_Power_Curve (KWh): double (nullable = true)\n",
      " |-- Wind Direction (°): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eed38d-1766-4a36-8f05-dac21b61ba20",
   "metadata": {},
   "source": [
    "Convert the joined DataFrame to JSON: we select all columns and nest them in a JSON object named \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8263b76-74dd-4d0b-8da5-979f2e648e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = joined_df.select(\n",
    "    to_json(struct([col(x) for x in joined_df.columns])).alias(\"value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f59fdc83-861b-482c-ac73-3fe0f8744ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79065b-ebc0-450a-893e-838404f125ec",
   "metadata": {},
   "source": [
    "Write the JSON objects to Kafka:\n",
    "- `kafka.bootstrap.servers` must be set to the Kafka service inside Docker\n",
    "- `checkpointLocation` is where Spark stores offsets and progress\n",
    "- `trigger(once=True)` runs the stream once and then terminates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e9b3e0a-d354-473c-bb30-8d8ceb174e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = out.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"topic\", \"xenon-topic\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kafka_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98df071-9c8f-449f-b9c3-e2955a02a462",
   "metadata": {},
   "source": [
    "Wait for streaming to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "434eb095-4bd6-4fc6-88dc-e4c3fcd19172",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925ff55-91f4-444c-ba45-1315a6162038",
   "metadata": {},
   "source": [
    "Stop the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d768b412-6eed-4665-a5c4-c1a388e258f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sprk.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
